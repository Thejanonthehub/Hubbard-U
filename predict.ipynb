{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a0a0d0-c1f1-495a-88d5-9cd814fc1df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path to CIF file:  /Users/thejanhasaranga/Downloads/CaTiO3.cif\n",
      "Enter species to analyze (e.g., Ag, C, H):  Ti\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './engine/hubbard_uj_poly.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    132\u001b[39m fe = Fe_for_pred(cif_file=cif_file,\n\u001b[32m    133\u001b[39m                  output_csv=\u001b[33m\"\u001b[39m\u001b[33mpredict_features.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    134\u001b[39m                  target_species=target_species)\n\u001b[32m    135\u001b[39m X_input = fe.extract_features()\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m mlp_pred, rf_pred, ens_pred = \u001b[43mpredict_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPredicted Properties:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMLP Prediction (U, J): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlp_pred[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mpredict_values\u001b[39m\u001b[34m(X_input, alpha)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_values\u001b[39m(X_input, alpha=\u001b[32m0.7\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     poly, scaler, rf_model, mlp_model = \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     X_poly = poly.transform(X_input)\n\u001b[32m    116\u001b[39m     X_scaled = scaler.transform(X_poly)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mload_models\u001b[39m\u001b[34m(model_dir, prefix)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_models\u001b[39m(model_dir=\u001b[33m'\u001b[39m\u001b[33m./engine\u001b[39m\u001b[33m'\u001b[39m, prefix=\u001b[33m'\u001b[39m\u001b[33mhubbard_uj\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load trained models from the given directory.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     poly = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_poly.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     scaler = joblib.load(os.path.join(model_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_scaler.pkl\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     84\u001b[39m     rf_model = joblib.load(os.path.join(model_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rf.pkl\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/jlab_env/lib/python3.11/site-packages/joblib/numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './engine/hubbard_uj_poly.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pymatgen.core import Structure, Element\n",
    "from pymatgen.analysis.local_env import CrystalNN\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction class\n",
    "# -----------------------------\n",
    "class Fe_for_pred:\n",
    "    def __init__(self, cif_file, output_csv, target_species, flush_interval=1):\n",
    "        self.cif_file = cif_file\n",
    "        self.output_csv = output_csv\n",
    "        self.target_species = target_species\n",
    "        self.flush_interval = flush_interval\n",
    "        self.features_list = []\n",
    "\n",
    "    def get_atomic_properties(self, species):\n",
    "        el = Element(species)\n",
    "        return {\n",
    "            \"electronegativity\": el.X,\n",
    "            \"ionization_energy\": el.ionization_energy,\n",
    "            \"electron_affinity\": el.electron_affinity,\n",
    "        }\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"Extract features from a single CIF file.\"\"\"\n",
    "        if not os.path.exists(self.cif_file):\n",
    "            raise FileNotFoundError(f\"No CIF file found at {self.cif_file}\")\n",
    "\n",
    "        structure = Structure.from_file(self.cif_file)\n",
    "\n",
    "        species = self.target_species\n",
    "        atomic_props = self.get_atomic_properties(species)\n",
    "\n",
    "        # Neighbor analysis using CrystalNN\n",
    "        try:\n",
    "            cnn = CrystalNN()\n",
    "            avg_nn_dist = np.mean([np.mean([np.linalg.norm(n['site'].coords - structure[i].coords)\n",
    "                                           for n in cnn.get_nn_info(structure, i)])\n",
    "                                   for i, site in enumerate(structure.sites)\n",
    "                                   if site.specie.symbol == species])\n",
    "        except:\n",
    "            avg_nn_dist = None\n",
    "\n",
    "        # Sum of neighbor atomic numbers\n",
    "        try:\n",
    "            s_stm_v = 0\n",
    "            for i, site in enumerate(structure.sites):\n",
    "                if site.specie.symbol == species:\n",
    "                    neighbors = cnn.get_nn_info(structure, i)\n",
    "                    s_stm_v += sum(nei['site'].specie.number for nei in neighbors)\n",
    "        except:\n",
    "            s_stm_v = None\n",
    "\n",
    "        feature_dict = {\n",
    "            \"Species\": species,\n",
    "            \"avg_nn_dist\": avg_nn_dist,\n",
    "            \"sum_atn_nn\": s_stm_v,\n",
    "            \"electronegativity\": atomic_props[\"electronegativity\"],\n",
    "            \"ionization_energy\": atomic_props[\"ionization_energy\"],\n",
    "            \"electron_affinity\": atomic_props[\"electron_affinity\"],\n",
    "        }\n",
    "\n",
    "        self.features_list.append(feature_dict)\n",
    "        df_features = pd.DataFrame(self.features_list)\n",
    "        df_features.to_csv(self.output_csv, index=False)\n",
    "\n",
    "        # Return features as numpy array in correct order\n",
    "        return np.array([avg_nn_dist, s_stm_v,\n",
    "                         atomic_props[\"electronegativity\"],\n",
    "                         atomic_props[\"ionization_energy\"],\n",
    "                         atomic_props[\"electron_affinity\"]]).reshape(1, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Load saved models\n",
    "# -----------------------------\n",
    "def load_models(model_dir='./engine', prefix='hubbard_uj'):\n",
    "    \"\"\"Load trained models from the given directory.\"\"\"\n",
    "    poly = joblib.load(os.path.join(model_dir, f'{prefix}_poly.pkl'))\n",
    "    scaler = joblib.load(os.path.join(model_dir, f'{prefix}_scaler.pkl'))\n",
    "    rf_model = joblib.load(os.path.join(model_dir, f'{prefix}_rf.pkl'))\n",
    "\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size=128, output_size=2, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc4 = torch.nn.Linear(hidden_size, output_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            return x\n",
    "\n",
    "    input_size = poly.transform(np.zeros((1,5))).shape[1]\n",
    "    mlp_model = MLP(input_size=input_size)\n",
    "    mlp_model.load_state_dict(torch.load(os.path.join(model_dir, f'{prefix}_mlp.pth')))\n",
    "    mlp_model.eval()\n",
    "    return poly, scaler, rf_model, mlp_model\n",
    "\n",
    "# -----------------------------\n",
    "# Predict values\n",
    "# -----------------------------\n",
    "def predict_values(X_input, alpha=0.7):\n",
    "    poly, scaler, rf_model, mlp_model = load_models()\n",
    "    X_poly = poly.transform(X_input)\n",
    "    X_scaled = scaler.transform(X_poly)\n",
    "    \n",
    "    y_rf = rf_model.predict(X_scaled)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        y_mlp = mlp_model(X_tensor).numpy()\n",
    "    y_ens = alpha * y_rf + (1 - alpha) * y_mlp\n",
    "    return y_mlp, y_rf, y_ens\n",
    "\n",
    "# -----------------------------\n",
    "# Main script\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    cif_file = input(\"Enter path to CIF file: \").strip()\n",
    "    target_species = input(\"Enter species to analyze (e.g., Ag, C, H): \").strip()\n",
    "\n",
    "    fe = Fe_for_pred(cif_file=cif_file,\n",
    "                     output_csv=\"predict_features.csv\",\n",
    "                     target_species=target_species)\n",
    "    X_input = fe.extract_features()\n",
    "\n",
    "    mlp_pred, rf_pred, ens_pred = predict_values(X_input)\n",
    "\n",
    "    print(\"\\nPredicted Properties:\")\n",
    "    print(f\"MLP Prediction (U, J): {mlp_pred[0]}\")\n",
    "    print(f\"RF Prediction (U, J): {rf_pred[0]}\")\n",
    "    print(f\"Ensemble Prediction (U, J): {ens_pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf6237-42e8-43f8-8605-5fdff830bffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
